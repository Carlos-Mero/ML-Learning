# 第一章节·学习笔记

> 笔记中大量内容由copilot生成。

d2l应该是业界最著名的开源教材之一……它最大的特点是结合了机器学习的理论并提供了丰富的实践范例，用来作为入门学习以及基本实操体验效果非常棒。在开源社区当中，有数百位撰稿人共同撰写维护，也正是这些贡献者保证了d2l中内容以及相关代码的质量以及时效性。

我们会在这里预览这本教材的前面一大部分内容，直到第一部分预备知识结束。

## 环境配置

因为这本教材当中有很多实践性的内容，在开始学习书中具体内容之前首先还得做好环境配置。

与大多数深度学习实践的做法相同，本书中主要采用python语言编写脚本，且大部分代码主要基于开源深度学习框架PyTorch。
本文中也会使用到很多其他库当中的函数和类，本书的创作团队为了方便使用将它们统一封装进了d2l这一个包中。

首先我们需要在计算机中安装Python，这里我已经拥有一个python环境，通过命令行显示相关信息如下：

```shell
python3.10 --version
Python 3.10.6
```

接着我们依次安装所需的各种外部库，通过如下指令：

```shell
pip install torch
pip install torchvision
pip install d2l
```

这就完成了我们所需环境的配置工作。

后续我们可能会使用到Jupyter Notebook等其他工具，这些就等到需要使用的时候再进行配置吧。

## 机器学习中的基本概念

机器学习主要用于处理通常直接编写的程序所难以完成的任务。它首先在一个通用的模型当中预留大量的参数，通过大量的数据集进行学习改良，最终获得处理问题的能力。在这一过程中，我们并不直接编写所需的程序，而是编写了一个“学习”的程序，让它自己去学习如何处理问题。

绝大多数的机器学习问题当中都会遇到一些关键组件：

* 训练数据集：用于训练模型的数据集（data）。
* 如何转换数据：对数据进行操作的模型（model）。
* 如何评估模型：一个目标函数（objective function），用来量化模型的有效性。
* 如何改进模型：一个优化算法（optimization algorithm），用来改进模型的参数。

针对数据集中是否带有标签，机器学习问题可以分为监督学习（supervised learning）和无监督学习（unsupervised learning）两种。例如回归判断就是一个非常经典的监督学习问题，而聚类则是一个非常经典的无监督学习问题。
除此以外，还有半监督学习（semi-supervised learning）和强化学习（reinforcement learning）等等其他类型的机器学习问题。

当前计算机存取存储器的发展速度有些落后于数据增长的速度，与此同时算力的增长速的则要超过数据。
如此一来现代机器学习框架所受到的最大限制就来源于内存上，因此需要想方设法提高内存效率。
因此我们会开始转向深度学习网络，通过在数据当中加入大量非线性项等等方式提高算力的利用率，并取得了机器学习的巨大进步。

## 机器学习的预备知识：

### 基本数据结构与操作 

通常来说我们需要做两件重要的事：

* 获取数据。
* 对数据进行操作。

深度学习框架PyTroch为我们提供了一个非常重要的数据结构，即`torch.Tensor`，它是一个多维数组，可以用来表示向量、矩阵、张量等等。
它最大的优势是可以调用GPU来进行计算，从而大大提高计算效率。同时它也兼容自动微分，能够为很多算法的编写提供方便。
接着我们在`code.py`当中编写一部分代码测试效果。

在pytorch当中，所有简单的四则运算以及求幂运算都进行了重载，因此我们可以直接使用`+`、`-`、`*`、`/`、`**`等符号来进行运算，这将会在两个张量之间进行对应元素的逐项运算。

与此同时，我们也有办法进行张量的拼接，通过调用`torch.cat`函数并指定维数，我们可以按照顺序将两个给定张量对应维数的元素首位相连在一起。

除此以外，取等、求和等运算也可以直接用。

然后是个大坑，广播机制。
广播机制是指当两个张量的形状不同时，将其中一个张量的形状进行扩展，使得两个张量的形状相同后再进行运算。这种扩展方式基本就是在行数不够的时候复制行，列数不够的时候复制列。张量的数乘实际上也可以看作是广播机制的一种特殊情况。

然后，pytorch里面的张量也可以像原生的数组一样支持索引和切片操作。

**接下来是相当重要的一点：节省内存**

因为我们知道深度学习中内存的瓶颈是很严重的，因此需要尽量地减少内存的使用，这个时候就要求我们在对张量进行操作的时候要尽可能地避免创建新的张量，而是直接对原有的张量进行操作。
示例如下：

```python
a = torch.ones(3)
b = torch.ones(3)
a += b
print(a)
```

### 数据预处理

Python当中最常用的数据处理软件包是pandas，其官方使用文档有千页之多，不过好在我们只需要使用我们需要的一部分即可。

我们首先需要创建一个人工数据集来进行测试操作，所使用的代码写在`data-g.py`当中。
对于这些手动输入的数据，我们可以采取一些简单的操作将它们区分为输入和输出两类数组，通过一些操作补全缺失数据，然后可以将其转换为pytorch当中可以处理的张量。

### 线性代数部分

这块就是简单的数学尝试，不必多提，简单看一下数学上的写法就好。

比较有趣的可能是矩阵转置的写法，直接调用`.T`即可。后面还有一些降维、求平均等操作，等到需要的时候再说，了解到有它们的存在即可。

更多测试代码见`code.py`。

### 微积分部分

同样都是些简单的数学基础，咱略不说。

### 自动微分

简而言之，因为微分运算在深度学习当中非常常见，而且手动算起来比较麻烦，因此pytorch提供了自动微分的功能，可以自动计算函数的梯度。

在这其中有一个重要的概念：反向传播（backpropagation）。
通过调用反向传播方法，我们可以跟踪某一个变量的整个计算过程（计算图），然后据此得到各个自变量对应的梯度。
详细代码如`diff.py`中所示。

值得注意的一点是，如果不采取额外操作，pytorch会记录下计算中得到的梯度并进行累加，因此在每次反向传播之前需要将梯度清零。这可以通过调用`.zero_()`方法来实现。

自动微分也可以计算相当复杂的分段函数等等，这在代码当中有使用到。

### 概率论基础

概率论是整个机器学习的基础。不同于计算机当中的其他算法，机器学习的正确性是依赖于概率的。
不过如果不做这方面的算法研究的话，了解一点概率论的基础知识即可。在这里我们通过调用`torch.distributions`包来进行概率分布的实验。

这里需要注意尽量不要使用python写算法，诸如for循环之类的东西都应该尽量交给pytorch来处理，因为python的运行效率实在是太低了。

（这里似乎遭遇了python某个类库中的bug，在执行目录下的`prob.py`文件时总会同时执行`code.py`，改名为`tensor.py`后可以规避掉。）

### 查阅文档

在这方面其实没有太多值得说的点。不过作为学习深度学习的笔记，在查阅文档方面其实可以尝试通过AI工具来完成。无论是ChatGPT或者是copilot都可以绕过文档直接通过注释快速编写代码，还是相当不错的。
