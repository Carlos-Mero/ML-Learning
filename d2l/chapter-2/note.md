# 第三章节·学习笔记

> 本文中大量内容由Copilot生成

## 线性神经网络

通常而言我们所说的线性神经网络中的技术类型主要包括线性回归和softmax回归。

回归是一种确定多个自变量和因变量之间关系建模的一类方法。上述的两种回归算法分别解决的就是对某些简单数值进行预测或者物品分类的问题。

### 线性回归

线性回归当中一般会假设因变量和自变量之间的关系是线性的，即因变量可以通过自变量的线性组合来表示。
它的出现早于一般机器学习的概念，由于这种线性的简单化假设，它主要适用于一些简单系统的计算拟合。

我们可以把线性回归假设写成一个矩阵的形式，即

$$\hat y=w^\tau x+b$$

其中，$\hat y$是预测值，$w$是权重，$x$是输入，$b$是偏置。

与此同时，我们时常会倾向于将已知的数据 $x$ 存储为一个矩阵 $X$，这会使得我们调用数据更加方便。
这会使用到广播机制进行更加方便的编程，写出表达式就是：

$$\bar y=Xw+b$$

我们的最终目标是寻找到更好的模型参数，即权重 $w$ 和偏置 $b$，使得预测值 $\hat y$ 更加接近真实值 $y$。
这一过程需要两个东西：损失函数和优化算法。
它们分别用于对模型的质量加以评判，以及对模型进行优化。

#### 损失函数

通常而言我们会选取非负数作为模型的损失，而模型优化的最终目标就是使得损失最小化。
对于线性回归而言，我们可以使用均方误差作为损失函数，即

$$l^{(i)}(w,b)=\frac{1}{2}(\hat y^{(i)}-y^{(i)})^2$$

这里度量的是第 $i$ 个样本的损失，而 $\hat y^{(i)}$ 是第 $i$ 个样本的预测值，$y^{(i)}$ 是第 $i$ 个样本的真实值。
更进一步地，我们需要对所有样本的损失进行求和与平均，得到整个数据集的损失，即

$$L(w,b)=\frac{1}{n}\sum_{i=1}^n l^{(i)}(w,b)$$

我们接下来将会通过优化算法不断地寻找更好的参数 $w$ 和 $b$，使得损失函数 $L$ 最小化。

事实上对于线性回归而言，最优参数是具有解析解的。但是在机器学习领域，直接使用解析解并不实用，因为我们的模型往往非常复杂。
所以使用更加通用的优化算法会更有意义。

#### 优化算法

这里我们主要使用一种叫做**梯度下降**的优化算法，它通过计算损失函数关于模型参数的梯度，并不断地沿着梯度的反方向迭代参数，从而使得损失最小化。
它几乎可以优化任何一个深度学习模型。

由于一次计算所有样本的损失函数的计算量较大，所以我们通常会使用随机梯度下降（SGD）来进行优化。它通过每次随机抽取
一部分样本来计算损失函数的梯度，从而减少了计算量。
如果写成数学表达式，即

$$(w,b)-\frac{\eta}{|\mathcal B|}\sum_{i\in\mathcal B}\partial_{(w,b)}l^{i}(w,b)\to$$

其中，迭代使用的 $\eta$ 是一个预先确定的常量，称为学习率，$\mathcal B$ 是一个随机抽取的小批量样本集合，$|\mathcal B|$ 是小批量样本的数量。

上面的批量大小以及学习率都是超参数，意即它们不是通过模型学习得到的参数，而是需要我们人为地进行设置。
**调参**所指的就是认为调整选择超参数的过程。

通常对于一个大模型而言，其中损失函数的极小值不止一个，也极难通过数学方法或者训练方法得到，而我们通常也并不追求最好，只要做到损失更小就行。
一个更具挑战性同时也更有意义的问题在于如何使得模型能够在我们从未见过的数据集上实现较低的损失，这就是泛化能力。

最后，我们希望能够通过这一种模型来对未知的数据进行预测。

#### 矢量化加速

这是一种编写代码的技巧，在训练我们的模型时，我们会希望更多地调用现成的高效算法，在短时间内处理大批量的数据。
这就要求我们更多地使用线性代数库来实现高效的并行计算。

这里我们通过编写一段代码计时来展现这一点。具体内容放在`accelerate.py`中。

可以看到通过调用现成的高效算法可以使得数值计算的速度提升至少数千倍。

（当然，python语言自己的低效率肯定也得背锅。）

#### 正态分布与平方损失

首先我们有经典的正态分布概率密度函数：

$$p(x)=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

其中，$\mu$是均值，$\sigma$是标准差。

我们可以通过编写一个简单的python脚本来可视化地绘制出正态分布曲线。具体代码我们存放在`normal.py`中。

最终我们可以通过`matplotlib`绘制得出如下所示的正态分布曲线：

![Normal Distribution](https://cdn.jsdelivr.net/gh/Yellow-GGG/Pics@main/ml4zce.png)

我们通常会认定我们对于数据的观测值中包含了一个随机的误差项，即“噪声”。而这个噪声项通常服从正态分布。
后面我们可能会经常用到这些。

#### 线性回归与深度网络

深度学习从业者喜欢绘制图表来表示可视化模型中正在发生的事情。但是因为太麻烦我们就不画了x

以线性回归模型为例，它可以接受一系列的输入，并最终产生一个单一的输出值。其中，输入变量的数量称为输入的特征维度（feature dimensionality）。
由于这一模型当中仅有一层输入和输出，所以我们称之为**单层神经网络**。
除此以外，由于线性回归当中的每个输入都与每个输出相连，我们称之为**全连接层**（fully-connected layer）或者**稠密层**（dense layer）。

#### 生物学相关概念

深度学习当中的很多概念、名称都来自于生物学，但其实它们的含义和研究方法差异很大。
所以“神经网络”等概念更多只是一种简单的类比，不用想太多。

### 线性回归模型的具体实现

这里我们主要利用人工智能框架当中的张量、自动求导等功能来实现线性回归模型。
后面我们会有更加方便的操作，不过目前通过这些进行一个简单的训练也是不错的。

这里的主要代码放在`linear-regression.py`中。

#### 生成数据集

为简单起见，我们直接使用一个带有噪声的线性函数来生成一个人造的数据集。
我们最终希望通过有限个样本的数据集来训练恢复这个模型的参数。

这里我们选取这样的线性函数

$$\mathbf y=\mathbf{Xw}+b+\varepsilon$$

其中模型参数 $\mathbf w=[2,-3,4]^\tau,b=4.2$，噪声项 $\varepsilon$ 服从均值为0、标准差为0.01的正态分布(这种均值为零的正态分布被称为标准假设)。
在这里 $\mathbf X$ 是我们人为生成的数据集，包含1000个样本，每个样本包含两个特征。
我们将它写作一个矩阵的形式，即 $\mathbf X\in\mathbb R^{1000\times 2}$。

#### 读取数据

在训练模型的过程中，我们需要随机抽取一小部分样本进行训练，并以此对模型进行更新。
因此我们需要定义一个函数打乱集中的样本并以小批量的形式返回。参见代码当中的`data_iter`函数。

我们会通过控制每次迭代当中选取的批量大小来充分利用GPU并行运算的优势。
当然，实际上我们这里简单的迭代器直接将所有数据读取到了内存之中再使用，而且伴随有大量的内存随机读取，实际上这样是非常低效的。
后续直接调用深度学习框架当中提供的函数，这方面的效率会高得多。

#### 初始化模型参数

在通过小批量随机梯度下降算法优化模型参数之前，我们需要首先确定一个初始值（也就是需要有一些超参数）。
在这个模型当中，我们采用均值为0、标准差为0.01的正态分布随机初始化模型参数。当然后面手动赋值也是可以的。

#### 定义模型

接下来我们必须要确定模型是什么，如此才能真正将模型参数的输入和输出联系起来。
在这里我们就是最为简单的情形，线性回归模型的代码标识也相当简单：
```python
def linreg(X, w, b):
	"""线性回归模型"""
	return torch.matmul(X, w) + b
```
我们直接写到代码里面就好。

#### 定义损失函数

我们的损失函数就直接定义成均方损失即可，其中使用到了广播机制进行直接的计算。
```python
def squared_loss(y_hat, y):
	"""均方损失"""
	return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```
并没有什么特别的地方。

#### 定义优化算法

接下来一个人工智能模型最重要的组成部分还剩下一个优化算法。目前而言它可能是线性神经网络当中最复杂的部分了。

我们这里使用的是小批量随机梯度下降算法，它的基本思想是通过随机抽取小批量样本来近似整个数据集的梯度。
在那之后，我们会朝着减少损失的方向更新我们的参数。每一步更新的大小由学习率(learning rate)决定。
因为我们使用的是批量样本的损失总和，我们会根据批量大小对每一次的学习率进行规范化。
具体的代码参见`sgd`函数。

#### 训练模型

在完成了AI模型各个要素的定义之后，我们就可以开始模型的训练过程了。
AI的训练过程所做的就是不断进行迭代并寻找更好的模型参数。
在这里我们使用的是一个简单的迭代过程，每一次迭代都会使用一个小批量的样本来计算损失，此后通过反向传播，存储好每个参数的梯度之后再调用`sgd`算法来更新参数。

最后我们打印出训练得到的参数以及损失、误差情况，可以看出虽然只经过了三次迭代，我们的计算结果就已经相当接近真实的模型参数了。
```shell
epoch 1, loss 0.045172
epoch 2, loss 0.000168
epoch 3, loss 0.000044
error in estimating w: tensor([ 5.3048e-05, -7.6199e-04])
error in estimating b: tensor([0.0006])
```
实际上，在非常复杂的优化问题上，我们也可以通过这种随机梯度下降的方法来进行有效的优化，
并得到很多高度精确的预测。

### 线性回归的简洁实现

通过调用深度学习框架的API，我们可以更加简洁地实现各类人工智能模型。
它们可以自动化地实现很多AI算法中的重复性工作，除去已使用过的张量、自动微分以外，
还有数据迭代器、损失函数、优化器和神经网络层等等。
它们都是有内置在框架中的高效实现的，我们可以直接调用。

这里我们就通过更多地使用`PyTorch`的API来实现线性回归模型。具体代码放在`easy-linreg.py`文件中

值得注意的是这里的`is_train`参数，它表示的是是否希望数据迭代器在每个迭代周期内打乱数据。。

#### 定义模型阶段

在这里我们可以使用许多框架内预定义好的层，这使得我们只需要关注使用哪些层来构造模型即可。
这里我们使用`nn`模块中的`Sequential`类来构造一个线性回归模型（`nn`是神经网络的缩写）。

在这里，`Sequential`实例可以看作是一个串联各个层的容器。
由于当前我们的模型只有一层，因此实际上并不需要使用`Sequential`类。
但它是后面各项工作的基础，所以在这里首先熟悉一下它的基本使用方法会更好。

后续的许多工作都可以借由模型非常轻松地完成，见代码即可。
可见我们最终实现的效果已经相当不错了。

### softmax回归

softmax回归是分类算法中最简单的一种，它的基本思想是将输出层的输出值看作是样本属于各个类别的概率。
此处它的含义就可以理解为“软性类别”当中的最大概率。

#### 分类问题

现在统计学中常常会使用独热编码(one-hot encoding)的方式来表示分类数据。
一个独热编码是一个向量，它的维数和我们拥有的类别数相同。
其每一个维数上的数值都可以被理解为是属于当前类别的概率值。

#### 网络结构

典型的softmax回归也是一个线性模型，我们假设了每一个输出类别都和特征向量是线性相关的。
写出具体表达式的话大致如下：

$$\hat{y}_1, \hat{y}_2, \ldots, \hat{y}_k = \text{softmax}(\mathbf{W} \mathbf{x} + \mathbf{b})$$

这其中就使用到了softmax运算。
它是一种针对模型输出的规范化校准，使得各个输出值满足概率的基本公理假设。
也就是不能出现负值，且所有值的和为1。

具体而言它的计算也并不复杂，有一个简单的公式：

$$\text{softmax}(\mathbf{x})_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}$$

由于softmax运算并不会改变各个元素之间的大小关系，因此我们可以将它看作是一个归一化函数。
尽管softmax是一个非线性函数，但它最终的结果仅仅由输入特征的仿射变换决定，因此softmax回归仍然是一个线性模型。

#### 交叉熵损失函数

接下来我们需要针对这一模型给定一个损失函数。
这里我们将使用最大似然估计，它与线性回归当中使用的方法相同。
我们针对任意一个标签 $\mathbf y$ 以及以及对应的模型预测 $\mathbf{\hat y}$
我们都可以定义其损失函数为：

$$l(\mathbf y,\mathbf{\hat y})=- \sum_j y_j \log \hat{y}_j$$

这样的损失函数就被称为交叉熵损失函数(cross-entropy loss)。
注意到每一个数据值 $\mathbf y$ 都是一个独热编码向量，其中除去某一个类别的概率值为1之外，其余的值都为0。
因此上述求和中实际上就只有一项有意义。
与此同时也可以看到这个损失函数的值就是恒正的。

这里损失函数的导数也可以直接带入计算得到：

$$\partial_{o_j}l(\mathbf y,\mathbf{\hat y})=\frac{\exp(o_j)}{\sum_{k=1}^q\exp(o_k)}-y_j=\text{softmax}(\mathbf o)_j-y_j$$

其中 $\mathbf o$ 是未经规范化的模型输出，$\mathbf y$ 是相应的数据标签。

### softmax回归的实际操作

#### 获取和读取数据

这里我们使用一个经典的数据集：Fashion-MNIST。
整个读取数据集过程所使用的代码存放在`softmax.py`文件中。

这个数据集可以通过`torchvision`包来自动下载。

Fashion-MNIST包含了10个类别的共7万个灰度图像，其中每个类别包含了7千张图像。
这些图像都是28像素×28像素的。

我们将最终读取部分的代码整合放在`FM_load.py`当中供后来调用。

#### 从零开始的实现方式

本篇中具体代码如`softmax-1.py`中所示。

做法大致和线性回归类似，我们就不多赘述了。

这里有一点很不相同的就是分类精度问题。
它很难直接作为损失函数计算，因为它很多时候都不可导。
但我们会使用它来评价模型的好坏。
相关函数仍然写在代码当中。
